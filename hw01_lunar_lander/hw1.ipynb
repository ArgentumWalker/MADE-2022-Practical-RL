{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from gym import make\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "\n",
    "from train import set_seed, DQN, evaluate_policy, DeepQNetworkModel\n",
    "\n",
    "SEED = 42\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "INITIAL_STEPS = 1024\n",
    "TRANSITIONS = 500_000\n",
    "LEARNING_RATE = 5e-4\n",
    "HID_DIM = 64"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b23533aa",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f2cffab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/and/miniforge3/envs/rl/lib/python3.9/site-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n",
      "/home/and/miniforge3/envs/rl/lib/python3.9/site-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 5000/500000, Best reward mean: -inf, Reward mean: -108.50, Reward std: 64.67\n",
      "Step: 10000/500000, Best reward mean: -108.50, Reward mean: -186.47, Reward std: 53.22\n",
      "Step: 15000/500000, Best reward mean: -108.50, Reward mean: -204.06, Reward std: 142.76\n",
      "Step: 20000/500000, Best reward mean: -108.50, Reward mean: 3.61, Reward std: 136.35\n",
      "Step: 25000/500000, Best reward mean: 3.61, Reward mean: -164.20, Reward std: 158.91\n",
      "Step: 30000/500000, Best reward mean: 3.61, Reward mean: -163.38, Reward std: 66.36\n",
      "Step: 35000/500000, Best reward mean: 3.61, Reward mean: -86.18, Reward std: 37.95\n",
      "Step: 40000/500000, Best reward mean: 3.61, Reward mean: -43.23, Reward std: 78.89\n",
      "Step: 45000/500000, Best reward mean: 3.61, Reward mean: -44.61, Reward std: 154.63\n",
      "Step: 50000/500000, Best reward mean: 3.61, Reward mean: -98.33, Reward std: 138.40\n",
      "Step: 55000/500000, Best reward mean: 3.61, Reward mean: -58.85, Reward std: 6.04\n",
      "Step: 60000/500000, Best reward mean: 3.61, Reward mean: -205.78, Reward std: 43.11\n",
      "Step: 65000/500000, Best reward mean: 3.61, Reward mean: -98.30, Reward std: 31.85\n",
      "Step: 70000/500000, Best reward mean: 3.61, Reward mean: -115.48, Reward std: 79.88\n",
      "Step: 75000/500000, Best reward mean: 3.61, Reward mean: -81.12, Reward std: 44.45\n",
      "Step: 80000/500000, Best reward mean: 3.61, Reward mean: -93.02, Reward std: 39.15\n",
      "Step: 85000/500000, Best reward mean: 3.61, Reward mean: -57.55, Reward std: 38.33\n",
      "Step: 90000/500000, Best reward mean: 3.61, Reward mean: -186.88, Reward std: 33.71\n",
      "Step: 95000/500000, Best reward mean: 3.61, Reward mean: -194.79, Reward std: 18.96\n",
      "Step: 100000/500000, Best reward mean: 3.61, Reward mean: -84.50, Reward std: 19.46\n",
      "Step: 105000/500000, Best reward mean: 3.61, Reward mean: 2.35, Reward std: 116.75\n",
      "Step: 110000/500000, Best reward mean: 3.61, Reward mean: 64.74, Reward std: 108.38\n",
      "Step: 115000/500000, Best reward mean: 64.74, Reward mean: -53.99, Reward std: 15.11\n",
      "Step: 120000/500000, Best reward mean: 64.74, Reward mean: -35.09, Reward std: 12.94\n",
      "Step: 125000/500000, Best reward mean: 64.74, Reward mean: -28.81, Reward std: 29.01\n",
      "Step: 130000/500000, Best reward mean: 64.74, Reward mean: -28.03, Reward std: 19.09\n",
      "Step: 135000/500000, Best reward mean: 64.74, Reward mean: -27.98, Reward std: 14.68\n",
      "Step: 140000/500000, Best reward mean: 64.74, Reward mean: -15.16, Reward std: 11.82\n",
      "Step: 145000/500000, Best reward mean: 64.74, Reward mean: -184.92, Reward std: 158.90\n",
      "Step: 150000/500000, Best reward mean: 64.74, Reward mean: -0.15, Reward std: 88.96\n",
      "Step: 155000/500000, Best reward mean: 64.74, Reward mean: -48.69, Reward std: 117.58\n",
      "Step: 160000/500000, Best reward mean: 64.74, Reward mean: 64.56, Reward std: 93.59\n",
      "Step: 165000/500000, Best reward mean: 64.74, Reward mean: 70.48, Reward std: 93.61\n",
      "Step: 170000/500000, Best reward mean: 70.48, Reward mean: 142.52, Reward std: 76.37\n",
      "Step: 175000/500000, Best reward mean: 142.52, Reward mean: 29.40, Reward std: 63.73\n",
      "Step: 180000/500000, Best reward mean: 142.52, Reward mean: -4.46, Reward std: 23.28\n",
      "Step: 185000/500000, Best reward mean: 142.52, Reward mean: 24.55, Reward std: 72.73\n",
      "Step: 190000/500000, Best reward mean: 142.52, Reward mean: 135.57, Reward std: 66.53\n",
      "Step: 195000/500000, Best reward mean: 142.52, Reward mean: 42.60, Reward std: 43.19\n",
      "Step: 200000/500000, Best reward mean: 142.52, Reward mean: 94.05, Reward std: 62.48\n",
      "Step: 205000/500000, Best reward mean: 142.52, Reward mean: 30.88, Reward std: 23.47\n",
      "Step: 210000/500000, Best reward mean: 142.52, Reward mean: -8.13, Reward std: 10.63\n",
      "Step: 215000/500000, Best reward mean: 142.52, Reward mean: -18.49, Reward std: 6.60\n",
      "Step: 220000/500000, Best reward mean: 142.52, Reward mean: 171.37, Reward std: 43.06\n",
      "Step: 225000/500000, Best reward mean: 171.37, Reward mean: 117.28, Reward std: 55.06\n",
      "Step: 230000/500000, Best reward mean: 171.37, Reward mean: 73.00, Reward std: 136.25\n",
      "Step: 235000/500000, Best reward mean: 171.37, Reward mean: 67.17, Reward std: 109.50\n",
      "Step: 240000/500000, Best reward mean: 171.37, Reward mean: -118.11, Reward std: 85.98\n",
      "Step: 245000/500000, Best reward mean: 171.37, Reward mean: 163.61, Reward std: 83.51\n",
      "Step: 250000/500000, Best reward mean: 171.37, Reward mean: 22.21, Reward std: 115.99\n",
      "Step: 255000/500000, Best reward mean: 171.37, Reward mean: 145.17, Reward std: 56.64\n",
      "Step: 260000/500000, Best reward mean: 171.37, Reward mean: -83.28, Reward std: 73.67\n",
      "Step: 265000/500000, Best reward mean: 171.37, Reward mean: 72.14, Reward std: 133.58\n",
      "Step: 270000/500000, Best reward mean: 171.37, Reward mean: 94.52, Reward std: 97.33\n",
      "Step: 275000/500000, Best reward mean: 171.37, Reward mean: 200.06, Reward std: 28.09\n",
      "Step: 280000/500000, Best reward mean: 200.06, Reward mean: 219.19, Reward std: 29.71\n",
      "Step: 285000/500000, Best reward mean: 219.19, Reward mean: 80.52, Reward std: 59.11\n",
      "Step: 290000/500000, Best reward mean: 219.19, Reward mean: 200.02, Reward std: 68.66\n",
      "Step: 295000/500000, Best reward mean: 219.19, Reward mean: 184.16, Reward std: 21.30\n",
      "Step: 300000/500000, Best reward mean: 219.19, Reward mean: 100.32, Reward std: 156.17\n",
      "Step: 305000/500000, Best reward mean: 219.19, Reward mean: 116.60, Reward std: 134.96\n",
      "Step: 310000/500000, Best reward mean: 219.19, Reward mean: 152.87, Reward std: 108.23\n",
      "Step: 315000/500000, Best reward mean: 219.19, Reward mean: 177.68, Reward std: 108.39\n",
      "Step: 320000/500000, Best reward mean: 219.19, Reward mean: 142.45, Reward std: 115.58\n",
      "Step: 325000/500000, Best reward mean: 219.19, Reward mean: -11.13, Reward std: 91.00\n",
      "Step: 330000/500000, Best reward mean: 219.19, Reward mean: 115.03, Reward std: 113.56\n",
      "Step: 335000/500000, Best reward mean: 219.19, Reward mean: 130.14, Reward std: 102.65\n",
      "Step: 340000/500000, Best reward mean: 219.19, Reward mean: 173.00, Reward std: 56.23\n",
      "Step: 345000/500000, Best reward mean: 219.19, Reward mean: 158.70, Reward std: 90.80\n",
      "Step: 350000/500000, Best reward mean: 219.19, Reward mean: 188.39, Reward std: 24.79\n",
      "Step: 355000/500000, Best reward mean: 219.19, Reward mean: 215.09, Reward std: 25.62\n",
      "Step: 360000/500000, Best reward mean: 219.19, Reward mean: 151.49, Reward std: 93.94\n",
      "Step: 365000/500000, Best reward mean: 219.19, Reward mean: 78.99, Reward std: 114.79\n",
      "Step: 370000/500000, Best reward mean: 219.19, Reward mean: 175.73, Reward std: 66.64\n",
      "Step: 375000/500000, Best reward mean: 219.19, Reward mean: 181.93, Reward std: 45.06\n",
      "Step: 380000/500000, Best reward mean: 219.19, Reward mean: 123.82, Reward std: 86.51\n",
      "Step: 385000/500000, Best reward mean: 219.19, Reward mean: 201.00, Reward std: 52.06\n",
      "Step: 390000/500000, Best reward mean: 219.19, Reward mean: 115.44, Reward std: 121.06\n",
      "Step: 395000/500000, Best reward mean: 219.19, Reward mean: 186.53, Reward std: 51.31\n",
      "Step: 400000/500000, Best reward mean: 219.19, Reward mean: 154.46, Reward std: 84.03\n",
      "Step: 405000/500000, Best reward mean: 219.19, Reward mean: 110.22, Reward std: 66.70\n",
      "Step: 410000/500000, Best reward mean: 219.19, Reward mean: 235.11, Reward std: 16.70\n",
      "Step: 415000/500000, Best reward mean: 235.11, Reward mean: 255.83, Reward std: 19.88\n",
      "Step: 420000/500000, Best reward mean: 255.83, Reward mean: 209.47, Reward std: 3.56\n",
      "Step: 425000/500000, Best reward mean: 255.83, Reward mean: 102.10, Reward std: 117.59\n",
      "Step: 430000/500000, Best reward mean: 255.83, Reward mean: 183.65, Reward std: 44.67\n",
      "Step: 435000/500000, Best reward mean: 255.83, Reward mean: 151.62, Reward std: 57.71\n",
      "Step: 440000/500000, Best reward mean: 255.83, Reward mean: 252.14, Reward std: 13.60\n",
      "Step: 445000/500000, Best reward mean: 255.83, Reward mean: 218.66, Reward std: 16.89\n",
      "Step: 450000/500000, Best reward mean: 255.83, Reward mean: 250.55, Reward std: 21.36\n",
      "Step: 455000/500000, Best reward mean: 255.83, Reward mean: 184.68, Reward std: 91.53\n",
      "Step: 460000/500000, Best reward mean: 255.83, Reward mean: 199.75, Reward std: 62.48\n",
      "Step: 465000/500000, Best reward mean: 255.83, Reward mean: 117.89, Reward std: 185.59\n",
      "Step: 470000/500000, Best reward mean: 255.83, Reward mean: 202.75, Reward std: 69.90\n",
      "Step: 475000/500000, Best reward mean: 255.83, Reward mean: 248.03, Reward std: 11.55\n",
      "Step: 480000/500000, Best reward mean: 255.83, Reward mean: 185.07, Reward std: 151.73\n",
      "Step: 485000/500000, Best reward mean: 255.83, Reward mean: 276.12, Reward std: 18.70\n",
      "Step: 490000/500000, Best reward mean: 276.12, Reward mean: 237.72, Reward std: 22.01\n",
      "Step: 495000/500000, Best reward mean: 276.12, Reward mean: 242.51, Reward std: 20.94\n",
      "Step: 500000/500000, Best reward mean: 276.12, Reward mean: 251.29, Reward std: 31.11\n"
     ]
    }
   ],
   "source": [
    "set_seed(SEED)\n",
    "\n",
    "env = make(\"LunarLander-v2\")\n",
    "dqn = DQN(state_dim=env.observation_space.shape[0], action_dim=env.action_space.n, hid_dim=HID_DIM)\n",
    "eps = 0.1\n",
    "state = env.reset()\n",
    "\n",
    "for _ in range(INITIAL_STEPS):\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    next_state, reward, done, *_ = env.step(action)\n",
    "    dqn.consume_transition((state, action, next_state, reward, done))\n",
    "\n",
    "    state = next_state if not done else env.reset()\n",
    "\n",
    "best_avg_rewards = -np.inf\n",
    "# pbar = tqdm(total=TRANSITIONS)\n",
    "for i in range(TRANSITIONS):\n",
    "    # Epsilon-greedy policy\n",
    "    if random.random() < eps:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        action = dqn.act(state)\n",
    "\n",
    "    next_state, reward, done, *_ = env.step(action)\n",
    "    dqn.update((state, action, next_state, reward, done))\n",
    "\n",
    "    state = next_state if not done else env.reset()\n",
    "\n",
    "    # pbar.update(1)\n",
    "\n",
    "    if (i + 1) % (TRANSITIONS // 100) == 0:\n",
    "        rewards = evaluate_policy(dqn, 5)\n",
    "        avg_reward = np.mean(rewards)\n",
    "        # pbar.set_description(\n",
    "        #     f\"Best reward mean: {best_avg_rewards:.2f}, Reward mean: {avg_reward:.2f}, Reward std: {np.std(rewards):.2f}\"\n",
    "        # )\n",
    "        print(f\"Step: {i + 1}/{TRANSITIONS}, Best reward mean: {best_avg_rewards:.2f}, Reward mean: {avg_reward:.2f}, Reward std: {np.std(rewards):.2f}\")\n",
    "        if avg_reward > best_avg_rewards:\n",
    "            best_avg_rewards = avg_reward\n",
    "            dqn.save()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34676cee",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "978ae78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self):\n",
    "        self.model = DeepQNetworkModel(8, 4, 64)\n",
    "        weights = torch.load(\"agent.pth\")\n",
    "        self.model.load_state_dict(weights)\n",
    "        self.model.to(DEVICE)\n",
    "        self.model.eval()\n",
    "\n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            action = np.argmax(self.model(state).cpu().numpy())\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7f47af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "251.3680759931396\n"
     ]
    }
   ],
   "source": [
    "rewards = evaluate_policy(Agent(), 50)\n",
    "print(np.mean(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e73794f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "68e00feeaafb6a0f333a17a108be481393c569b415f1e7f2c1cc8f535af0e323"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
